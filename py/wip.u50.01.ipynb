{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5a330012057cc7f58e3a64fbe36cf08498cc97b"
   },
   "source": [
    "In this Kernel I show you how to use pre-trained Resnet50 with image size 128. \n",
    "\n",
    "Thanks to everyone for sharing their insights. \n",
    "Please check out the following kernels for more ideas:\n",
    "\n",
    "Jack (Jiaxin) Shao: https://www.kaggle.com/shaojiaxin/u-net-with-simple-resnet-blocks-v2-new-loss\n",
    "\n",
    "Bruno G. do Amaral: https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\n",
    "\n",
    "Peter HÃ¶nigschmid: https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\n",
    "\n",
    "Juan C EsquivelTGS: https://www.kaggle.com/jcesquiveld/tgs-vanilla-u-net-with-simple-augmentation\n",
    "\n",
    "NPHard: https://www.kaggle.com/meaninglesslives/apply-crf-unet-resnet\n",
    "\n",
    "PS - You can easily get around 0.82 LB Score with some modifications. Happy Kaggling :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy --user\n",
    "!pip install pandas --user\n",
    "!pip install random --user\n",
    "!pip install matplotlib --user\n",
    "!pip install seaborn --user\n",
    "!pip install sklearn --user\n",
    "!pip install scikit-image --user\n",
    "!pip install tensorflow --user\n",
    "!pip install keras --user\n",
    "!pip install tqdm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --user\n",
    "!pip install scikit-image -U --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras import backend as K\n",
    "#from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"
   },
   "source": [
    "# Params and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e54e151245d665e42bb95d9cf2e1a33cb9440e48"
   },
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "530c358f2868a444e8233936996463a66c2cc4f3"
   },
   "source": [
    "# Loading of training/testing ids and depths\n",
    "Reading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24d7f3d982bfa582b222f012129acdda55282b6d"
   },
   "source": [
    "# Read images and masks\n",
    "Load the images and masks into the DataFrame and divide the pixel values by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b18c1f50cefd7504eae7e7b9605be3814c7cad6d"
   },
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86620c6a070571895f4f36ec050a25803915ed74"
   },
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1137f0a009f10b5f69e4dade5f689e744e9ce1d6"
   },
   "source": [
    "# Calculating the salt coverage and salt coverage classes\n",
    "Counting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\n",
    "Plotting the distribution of coverages and coverage classes, and the class against the raw coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18d2aa182a44c65a87c75f41047c653a79bc1c3f"
   },
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b13d1ecc7004832e8e042d034922796263054b7"
   },
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"
   },
   "source": [
    "# Show some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a6bc85ee458f72c0917edf77895d5abc5eaf3ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(train_df.index[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00655e32f93f96ebd90dbe94e35ee052f52217cd"
   },
   "source": [
    "# Create train/validation split stratified by salt coverage\n",
    "Using the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d3c3157512d11e71ac74ce51a937b85bedfe1d1"
   },
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2f1ab00f03e71e6d7f9b2214408b5a9779fc235"
   },
   "outputs": [],
   "source": [
    "tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\n",
    "tmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\n",
    "fix, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].imshow(tmp_img, cmap=\"Greys\")\n",
    "axs[0].set_title(\"Original image\")\n",
    "axs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\n",
    "axs[1].set_title(\"Scaled image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63ac58ab47921b4e4f54102e2c8b85fa318225f1"
   },
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "227c99b692237b3f6975b1f1ec154dc6ce09641d"
   },
   "outputs": [],
   "source": [
    "def conv_block_simple(prevlayer, filters, prefix, strides=(1, 1)):\n",
    "    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = BatchNormalization(name=prefix + \"_bn\")(conv)\n",
    "    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n",
    "    return conv\n",
    "\n",
    "def conv_block_simple_no_bn(prevlayer, filters, prefix, strides=(1, 1)):\n",
    "    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e873c8adbe616286d19d08d52d226fe41123457"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def get_unet_resnet(input_shape):\n",
    "    resnet_base = ResNet50(input_shape=input_shape, include_top=False)\n",
    "\n",
    "    for l in resnet_base.layers:\n",
    "        l.trainable = True\n",
    "    conv1 = resnet_base.get_layer(\"activation_1\").output\n",
    "    conv2 = resnet_base.get_layer(\"activation_10\").output\n",
    "    conv3 = resnet_base.get_layer(\"activation_22\").output\n",
    "    conv4 = resnet_base.get_layer(\"activation_40\").output\n",
    "    conv5 = resnet_base.get_layer(\"activation_49\").output\n",
    "\n",
    "    up6 = concatenate([UpSampling2D()(conv5), conv4], axis=-1)\n",
    "    conv6 = conv_block_simple(up6, 256, \"conv6_1\")\n",
    "    conv6 = conv_block_simple(conv6, 256, \"conv6_2\")\n",
    "\n",
    "    up7 = concatenate([UpSampling2D()(conv6), conv3], axis=-1)\n",
    "    conv7 = conv_block_simple(up7, 192, \"conv7_1\")\n",
    "    conv7 = conv_block_simple(conv7, 192, \"conv7_2\")\n",
    "\n",
    "    up8 = concatenate([UpSampling2D()(conv7), conv2], axis=-1)\n",
    "    conv8 = conv_block_simple(up8, 128, \"conv8_1\")\n",
    "    conv8 = conv_block_simple(conv8, 128, \"conv8_2\")\n",
    "\n",
    "    up9 = concatenate([UpSampling2D()(conv8), conv1], axis=-1)\n",
    "    conv9 = conv_block_simple(up9, 64, \"conv9_1\")\n",
    "    conv9 = conv_block_simple(conv9, 64, \"conv9_2\")\n",
    "\n",
    "    up10 = UpSampling2D()(conv9)\n",
    "    conv10 = conv_block_simple(up10, 32, \"conv10_1\")\n",
    "    conv10 = conv_block_simple(conv10, 32, \"conv10_2\")\n",
    "    conv10 = SpatialDropout2D(0.2)(conv10)\n",
    "    x = Conv2D(1, (1, 1), activation=\"sigmoid\", name=\"prediction\")(conv10)\n",
    "    model = Model(resnet_base.input, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f3e540ad1511f8bd7d7fe3a30d80d8112d406a0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.layers import Input\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'keras.., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'keras.., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n",
    "    And the shortcut should have strides=(2,2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                      name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "419366fb3ba6d766bbd60bedc8eb137b01cdcf24"
   },
   "outputs": [],
   "source": [
    "def ResNet50(include_top=True, weights='imagenet',\n",
    "             input_tensor=None, input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000):\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(img_input)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "#     x = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "\n",
    "#     if include_top:\n",
    "#         x = Flatten()(x)\n",
    "#         x = Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "#     else:\n",
    "#         if pooling == 'avg':\n",
    "#             x = GlobalAveragePooling2D()(x)\n",
    "#         elif pooling == 'max':\n",
    "#             x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnet50')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models',\n",
    "                                    md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n",
    "        else:\n",
    "            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models',\n",
    "                                    md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "        model.load_weights(weights_path,by_name=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7fb559b985c707bed36d802b3bde59f9df4e78c"
   },
   "outputs": [],
   "source": [
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2382b088c8a6be16490354ebd386120a9ced414d"
   },
   "outputs": [],
   "source": [
    "# model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)\n",
    "# model = get_unet_mobilenet(input_shape=(128,128,1))\n",
    "model = get_unet_resnet(input_shape=(img_size_target,img_size_target,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "384e0e560ae48ec19b9a2de8fe08874ba52e6f2c"
   },
   "outputs": [],
   "source": [
    "def get_iou_vector(A, B):\n",
    "    batch_size = A.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch]>0, B[batch]>0        \n",
    "        intersection = np.logical_and(t, p)\n",
    "        union = np.logical_or(t, p)\n",
    "        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "\n",
    "    return np.mean(metric)\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
    "\n",
    "def my_iou_metric_2(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2b91e594b2ac2878e0b54cb9af4505177ef02dc"
   },
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = weight * (logit_y_pred * (1. - y_true) + \n",
    "                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c85e46529ad406250b87e8d07bc2581688f0cdfb"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3399029adb039b049e3d6ca01fef30ed8653482b"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=bce_dice_loss, optimizer='adam', metrics=[my_iou_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58456186be9ded1b694972951a12891d8ebee590"
   },
   "outputs": [],
   "source": [
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7040f72549212dd4f71c13dfbd8bf013481ea369"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n",
    "    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "fig.suptitle(\"Top row: original images, bottom row: augmented images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "801124eef687040237f0aa3176f997c4b73b531c"
   },
   "outputs": [],
   "source": [
    "x_train = np.repeat(x_train,3,axis=3)\n",
    "x_valid = np.repeat(x_valid,3,axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5a6b1abaa4681cba3b608bc5f33cf260370d82a"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1773642758da7b4480e0e48c045bd01ea3684ae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\"./keras.model\",monitor='val_my_iou_metric', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[model_checkpoint, reduce_lr],shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f26775ce2a534322ba6dbcb33e9f80ffa58f748"
   },
   "outputs": [],
   "source": [
    "del ids_train, x_train, y_train, cov_train,depth_train,depths_df,depth_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['my_iou_metric'][1:])\n",
    "plt.plot(history.history['val_my_iou_metric'][1:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('val_my_iou_metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c824f6bca47f051500966c433ce7fb5a9528f6d7"
   },
   "outputs": [],
   "source": [
    "# model = load_model(\"./keras.model\")\n",
    "# Load best model\n",
    "model.load_weights('./keras.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f168318eadb324daa8c020f0e3e0a24d82a464f"
   },
   "source": [
    "# Predict the validation set to do a sanity check\n",
    "Again plot some sample images including the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1368851b5b7e95c7e942de0c8e97788d1c7e214"
   },
   "outputs": [],
   "source": [
    "def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n",
    "    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n",
    "    preds_test1 = model.predict([x_test]).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test2_refect = model.predict([x_test_reflect]).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n",
    "    preds_avg = (preds_test1 +preds_test2)/2\n",
    "    return preds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46"
   },
   "outputs": [],
   "source": [
    "preds_valid = predict_result(model,x_valid,img_size_target)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "y_valid_ori = np.array([downsample(x) for x in y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b1198b6fb7369c3cfb70e68cd1b78d36aa188bc"
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(ids_valid[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    pred = preds_valid[i]\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd973023204ebf921fe1f23748856e6a6f692aa4",
    "collapsed": true
   },
   "source": [
    "# Scoring\n",
    "Score the model and do a threshold optimization by the best IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d261beec66b6867ac0d5c94684f12aa08b70d638"
   },
   "outputs": [],
   "source": [
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85f6d9567cec0ef8976730a6834b6569b6e108a0"
   },
   "outputs": [],
   "source": [
    "## Scoring for last model\n",
    "thresholds = np.linspace(0.3, 0.7, 31)\n",
    "ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "183d37ad32bc2f1f0d17a9538702c45a826ccefc"
   },
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a3840fb10bf1a7738a067890dd02f20483cd333"
   },
   "outputs": [],
   "source": [
    "threshold_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "423b3268c580dc1eae84f54deeeb0f691eff6028"
   },
   "source": [
    "# Another sanity check with adjusted threshold\n",
    "Again some sample images with the adjusted threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40c263765ac6d53a8c0c1361ff1e6f061eecf825"
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(ids_valid[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    pred = preds_valid[i]\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n",
    "    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "332a614c0ae837c115ec6563f355753ffbb8cd83"
   },
   "source": [
    "# Submission\n",
    "Load, predict and submit the test image predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b43a50c7febc9347d31e4e8150b8bd13bb4e3f1"
   },
   "outputs": [],
   "source": [
    "del x_valid, y_valid,preds_valid,y_valid_ori,train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c446252eb668f2ab00a30e356c037b07d55218d"
   },
   "outputs": [],
   "source": [
    "def rle_encode(im):\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "batch_size = 500\n",
    "preds_test = []\n",
    "i = 0\n",
    "while i < test_df.shape[0]:\n",
    "    print('Images Processed:',i)\n",
    "    index_val = test_df.index[i:i+batch_size]\n",
    "    depth_val = test_df.z[i:i+batch_size]\n",
    "    x_test = np.array([upsample(np.array(load_img(\"../input/tgs-salt-identification-challenge/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in (index_val)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    x_test = np.repeat(x_test,3,axis=3)\n",
    "    preds_test_temp = predict_result(model,x_test,img_size_target)\n",
    "    if i==0:\n",
    "        preds_test = preds_test_temp\n",
    "    else:\n",
    "        preds_test = np.concatenate([preds_test,preds_test_temp],axis=0)\n",
    "#     print(preds_test.shape)\n",
    "    i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0820b910b1c657e9cd89dc3cb578a0e1281cc586"
   },
   "outputs": [],
   "source": [
    "del x_test,preds_test_temp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e73c314502ee5cb14a165fcf8cd5565a6ccc5b3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "pred_dict = {idx: rle_encode(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"Usedtime = {t2-t1} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4243166f91c4bcb4da00208f4f53dd912dbb429f"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('resnet50_pretrained_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
